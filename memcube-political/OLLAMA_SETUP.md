# Ollama + BGE-M3 æ¨¡å‹è®¾ç½®æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬é¡¹ç›®é…ç½®ä½¿ç”¨æœ¬åœ°OllamaæœåŠ¡è¿è¡ŒBGE-M3 embeddingæ¨¡å‹ï¼Œæä¾›é«˜æ•ˆçš„ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–èƒ½åŠ›ã€‚

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

- Ollama (0.1.0+)
- bge-m3 æ¨¡å‹
- è‡³å°‘ 4GB RAM
- Docker æˆ–æœ¬åœ°å®‰è£…

## ğŸš€ å¿«é€Ÿè®¾ç½®

### 1. å®‰è£…Ollama

#### Windows
```bash
# ä¸‹è½½å¹¶å®‰è£…Ollama
# è®¿é—® https://ollama.com/download ä¸‹è½½Windowsç‰ˆæœ¬
# æˆ–ä½¿ç”¨winget
winget install Ollama.Ollama
```

#### macOS
```bash
# ä½¿ç”¨Homebrew
brew install ollama

# æˆ–ä¸‹è½½DMGæ–‡ä»¶
# https://ollama.com/download
```

#### Linux
```bash
# å®˜æ–¹å®‰è£…è„šæœ¬
curl -fsSL https://ollama.com/install.sh | sh

# æˆ–ä½¿ç”¨Docker
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

### 2. å¯åŠ¨OllamaæœåŠ¡

```bash
# Windows/macOS - ä»åº”ç”¨èœå•å¯åŠ¨
# æˆ–å‘½ä»¤è¡Œå¯åŠ¨
ollama serve

# Docker
docker start ollama
```

### 3. ä¸‹è½½BGE-M3æ¨¡å‹

```bash
# ä¸‹è½½BGE-M3æ¨¡å‹ï¼ˆä¸­æ–‡æ”¯æŒï¼‰
ollama pull bge-m3

# éªŒè¯æ¨¡å‹å®‰è£…
ollama list
```

### 4. æµ‹è¯•æ¨¡å‹

```bash
# æµ‹è¯•embeddingåŠŸèƒ½
curl http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "bge-m3",
    "prompt": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
  }'
```

## âš™ï¸ é…ç½®éªŒè¯

### 1. æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€

```bash
# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
curl http://localhost:11434/api/tags

# åº”è¯¥è¿”å›ç±»ä¼¼ä»¥ä¸‹å†…å®¹ï¼š
# {"models":[{"name":"bge-m3:latest","modified_at":"...","size":...}]}
```

### 2. éªŒè¯æ¨¡å‹å¯ç”¨æ€§

```bash
# åˆ—å‡ºå·²å®‰è£…çš„æ¨¡å‹
ollama list

# ç¡®ä¿bge-m3åœ¨åˆ—è¡¨ä¸­
# NAME            ID              SIZE    MODIFIED
# bge-m3:latest   abc123...       670MB   2025-11-10
```

### 3. æµ‹è¯•embeddingåŠŸèƒ½

```python
# ä½¿ç”¨Pythonæµ‹è¯•
import requests

response = requests.post(
    "http://localhost:11434/api/embeddings",
    json={
        "model": "bge-m3",
        "prompt": "é©¬å…‹æ€ä¸»ä¹‰æ˜¯ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰çš„ç†è®ºåŸºç¡€"
    }
)

if response.status_code == 200:
    embedding = response.json()['embedding']
    print(f"âœ… EmbeddingæˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
    print(f"å‰5ä¸ªç»´åº¦: {embedding[:5]}")
else:
    print(f"âŒ é”™è¯¯: {response.status_code}")
```

## ğŸ”§ é…ç½®æ–‡ä»¶

é¡¹ç›®çš„é…ç½®æ–‡ä»¶å·²è‡ªåŠ¨è®¾ç½®ä¸ºä½¿ç”¨Ollamaï¼š

```yaml
# config/config.yaml
embedding:
  model_name: "bge-m3"      # Ollamaæ¨¡å‹å
  model_type: "ollama"       # ä½¿ç”¨Ollamaåç«¯
  ollama_url: "http://localhost:11434"  # OllamaæœåŠ¡åœ°å€
  batch_size: 16
  device: "cpu"
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. Ollamaè¿æ¥å¤±è´¥
```
é”™è¯¯: æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡
è§£å†³:
- ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ
- æ£€æŸ¥ç«¯å£11434æ˜¯å¦è¢«å ç”¨
- éªŒè¯é˜²ç«å¢™è®¾ç½®
```

#### 2. æ¨¡å‹æœªæ‰¾åˆ°
```
é”™è¯¯: æ¨¡å‹ bge-m3 æœªåœ¨ollamaä¸­æ‰¾åˆ°
è§£å†³:
- è¿è¡Œ `ollama pull bge-m3` ä¸‹è½½æ¨¡å‹
- ä½¿ç”¨ `ollama list` ç¡®è®¤æ¨¡å‹å®‰è£…
```

#### 3. å†…å­˜ä¸è¶³
```
é”™è¯¯: å†…å­˜ä¸è¶³
è§£å†³:
- å‡å°‘batch_sizeé…ç½®
- å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„ç¨‹åº
- è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„å†…å­˜
```

#### 4. å“åº”æ…¢
```
é—®é¢˜: embeddingç”Ÿæˆé€Ÿåº¦æ…¢
è§£å†³:
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„CPU/GPUèµ„æº
- å‡å°‘batch_size
- æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½
```

### æ—¥å¿—æ£€æŸ¥

```bash
# æŸ¥çœ‹Ollamaæ—¥å¿—
# Windows: %USERPROFILE%\.ollama\logs
# macOS: ~/.ollama/logs
# Linux: ~/.ollama/logs

# æˆ–è€…æŸ¥çœ‹Dockeræ—¥å¿—
docker logs ollama
```

### ç½‘ç»œé—®é¢˜

```bash
# æµ‹è¯•è¿æ¥
curl -I http://localhost:11434/api/tags

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -an | grep 11434  # Linux/macOS
netstat -an | findstr 11434  # Windows
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹å¤„ç†ä¼˜åŒ–

```yaml
# config/config.yaml
embedding:
  batch_size: 32  # æ ¹æ®å†…å­˜è°ƒæ•´
```

### 2. å¹¶å‘æ§åˆ¶

```yaml
# config/config.yaml
concept_expansion:
  max_workers: 5   # å‡å°‘å¹¶å‘é¿å…Ollamaè¿‡è½½
```

### 3. ç¼“å­˜ç­–ç•¥

é¡¹ç›®ä¼šè‡ªåŠ¨ç¼“å­˜embeddingç»“æœï¼Œå‡å°‘é‡å¤è®¡ç®—ã€‚

## ğŸ”„ æ¨¡å‹ç®¡ç†

### æ›´æ–°æ¨¡å‹
```bash
# æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬
ollama pull bge-m3:latest

# æˆ–è€…æŒ‡å®šç‰ˆæœ¬
ollama pull bge-m3:v1.0
```

### åˆ é™¤æ¨¡å‹
```bash
# åˆ é™¤ä¸éœ€è¦çš„æ¨¡å‹
ollama rm bge-m3
```

### æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
```bash
# æŸ¥çœ‹æ¨¡å‹è¯¦ç»†ä¿¡æ¯
ollama show bge-m3
```

## ğŸ›ï¸ é«˜çº§é…ç½®

### 1. è‡ªå®šä¹‰Ollamaç«¯å£

```yaml
# config/config.yaml
embedding:
  ollama_url: "http://localhost:11435"  # è‡ªå®šä¹‰ç«¯å£
```

### 2. Dockeré…ç½®

```yaml
# docker-compose.yml
version: '3'
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_PARALLEL=2
    restart: unless-stopped

volumes:
  ollama:
```

### 3. ç¯å¢ƒå˜é‡

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_PARALLEL=2

# å¯åŠ¨æœåŠ¡
ollama serve
```

## ğŸ“š ç›¸å…³èµ„æº

- [Ollamaå®˜æ–¹æ–‡æ¡£](https://github.com/ollama/ollama)
- [BGE-M3æ¨¡å‹ä»‹ç»](https://huggingface.co/BAAI/bge-m3)
- [é¡¹ç›®é…ç½®æ–‡ä»¶](config/config.yaml)

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹ [Ollama GitHub Issues](https://github.com/ollama/ollama/issues)
2. æ£€æŸ¥ [BGE-M3æ–‡æ¡£](https://huggingface.co/BAAI/bge-m3)
3. æŸ¥çœ‹é¡¹ç›®æ—¥å¿—æ–‡ä»¶
4. æäº¤Issueåˆ°é¡¹ç›®ä»“åº“

---

*é…ç½®å®Œæˆåï¼Œæ‚¨å¯ä»¥è¿è¡Œ `python quick_start.py` éªŒè¯ç¯å¢ƒè®¾ç½®*